name: Update Forum Data

on:
  schedule:
    - cron: '0 */1 * * *'  # Run every 1 hours
  workflow_dispatch:  # Allow manual triggering

jobs:
  update-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 html5lib lxml pytz

      - name: Create or update scraper script
        run: |
          cat > ivelt_scraper.py << 'EOF'
          import requests
          from bs4 import BeautifulSoup
          import json
          import os
          import re
          import time
          import random
          from datetime import datetime
          import pytz

          class IveltScraper:
              def __init__(self, output_file):
                  self.base_url = "https://ivelt.com/forum"
                  self.output_file = output_file
                  self.session = requests.Session()
                  self.session.headers.update({
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                      'Accept-Language': 'he,en-US;q=0.9,en;q=0.8',
                  })
                  self.categories = []
                  self.forums = []
                  self.topics = []
                  self.posts = []
                  
                  # Create timezone object for formatting dates
                  self.timezone = pytz.timezone('Asia/Jerusalem')
              
              def scrape(self):
                  """Main scraping method that orchestrates the process"""
                  print(f"Starting to scrape {self.base_url}")
                  
                  # Scrape the index page to get categories and forums
                  self.scrape_index()
                  
                  # Limit the number of forums to scrape (for testing and to avoid rate limiting)
                  forums_to_scrape = self.forums[:10]  # Adjust based on your needs
                  
                  # Scrape each forum to get topics
                  for forum in forums_to_scrape:
                      print(f"Scraping forum: {forum['name']}")
                      self.scrape_forum(forum['id'], forum['url'])
                      time.sleep(random.uniform(2, 5))  # Random delay between requests
                  
                  # Limit the number of topics to scrape
                  topics_to_scrape = self.topics[:50]  # Adjust based on your needs
                  
                  # Scrape each topic to get posts
                  for topic in topics_to_scrape:
                      print(f"Scraping topic: {topic['title']}")
                      self.scrape_topic(topic['id'], topic['url'])
                      time.sleep(random.uniform(2, 5))  # Random delay between requests
                  
                  # Save the scraped data
                  self.save_data()
                  
                  print(f"Scraping completed. Data saved to {self.output_file}")
              
              def scrape_index(self):
                  """Scrape the forum index page to get categories and forums"""
                  response = self.session.get(f"{self.base_url}/index.php")
                  soup = BeautifulSoup(response.text, 'html5lib')
                  
                  # Extract categories and forums based on phpBB structure
                  categories = soup.select('.forabg')
                  
                  for idx, category in enumerate(categories):
                      category_header = category.select_one('.header')
                      if not category_header:
                          continue
                          
                      category_name_elem = category_header.select_one('.header-title a')
                      if not category_name_elem:
                          continue
                          
                      category_name = category_name_elem.text.strip()
                      category_id = f"cat_{idx + 1}"
                      
                      category_data = {
                          'id': category_id,
                          'name': category_name,
                          'forums': []
                      }
                      
                      forums = category.select('ul.topiclist.forums li.row')
                      for forum_idx, forum in enumerate(forums):
                          forum_title_elem = forum.select_one('.forumtitle')
                          if not forum_title_elem:
                              continue
                              
                          forum_title = forum_title_elem.text.strip()
                          forum_url = forum_title_elem['href']
                          if not forum_url.startswith('http'):
                              forum_url = f"{self.base_url}/{forum_url.lstrip('./')}"
                          
                          # Extract forum ID from URL
                          forum_id_match = re.search(r'f=(\d+)', forum_url)
                          forum_id = forum_id_match.group(1) if forum_id_match else f"forum_{category_id}_{forum_idx + 1}"
                          
                          forum_desc_elem = forum.select_one('.forum-description')
                          forum_desc = forum_desc_elem.text.strip() if forum_desc_elem else ""
                          
                          # Try to extract statistics
                          topics_count = 0
                          posts_count = 0
                          stats_elem = forum.select_one('.forum-statistics')
                          if stats_elem:
                              topics_match = re.search(r'(\d+)\s+טעמעס', stats_elem.text)
                              posts_match = re.search(r'(\d+)\s+תגובות', stats_elem.text)
                              topics_count = int(topics_match.group(1)) if topics_match else 0
                              posts_count = int(posts_match.group(1)) if posts_match else 0
                          
                          # Extract last post info if available
                          last_post_info = {
                              'title': '',
                              'author': '',
                              'date': '',
                              'url': ''
                          }
                          
                          last_post_elem = forum.select_one('.last-post')
                          if last_post_elem:
                              last_post_title_elem = last_post_elem.select_one('a:not(.username)')
                              if last_post_title_elem:
                                  last_post_info['title'] = last_post_title_elem.text.strip()
                                  last_post_info['url'] = last_post_title_elem['href']
                                  if not last_post_info['url'].startswith('http'):
                                      last_post_info['url'] = f"{self.base_url}/{last_post_info['url'].lstrip('./')}"
                              
                              last_post_author_elem = last_post_elem.select_one('.username')
                              if last_post_author_elem:
                                  last_post_info['author'] = last_post_author_elem.text.strip()
                              
                              last_post_date_elem = last_post_elem.select_one('time')
                              if last_post_date_elem:
                                  last_post_info['date'] = last_post_date_elem.text.strip()
                          
                          forum_data = {
                              'id': forum_id,
                              'name': forum_title,
                              'description': forum_desc,
                              'url': forum_url,
                              'category_id': category_id,
                              'topics_count': topics_count,
                              'posts_count': posts_count,
                              'last_post': last_post_info
                          }
                          
                          self.forums.append(forum_data)
                          category_data['forums'].append(forum_id)
                      
                      self.categories.append(category_data)
              
              def scrape_forum(self, forum_id, forum_url):
                  """Scrape a forum page to get topics"""
                  response = self.session.get(forum_url)
                  soup = BeautifulSoup(response.text, 'html5lib')
                  
                  # Extract topics
                  topics = soup.select('.topiclist.topics .row')
                  
                  for idx, topic in enumerate(topics):
                      topic_title_elem = topic.select_one('.topictitle')
                      if not topic_title_elem:
                          continue
                          
                      topic_title = topic_title_elem.text.strip()
                      topic_url = topic_title_elem['href']
                      if not topic_url.startswith('http'):
                          topic_url = f"{self.base_url}/{topic_url.lstrip('./')}"
                      
                      # Extract topic ID from URL
                      topic_id_match = re.search(r't=(\d+)', topic_url)
                      topic_id = topic_id_match.group(1) if topic_id_match else f"topic_{forum_id}_{idx + 1}"
                      
                      # Extract topic author
                      topic_author = ""
                      topic_author_elem = topic.select_one('.topic-poster .username')
                      if topic_author_elem:
                          topic_author = topic_author_elem.text.strip()
                      
                      # Extract topic date
                      topic_date = ""
                      topic_date_elem = topic.select_one('.topic-poster time')
                      if topic_date_elem:
                          topic_date = topic_date_elem.text.strip()
                      
                      # Extract topic statistics
                      replies_count = 0
                      views_count = 0
                      stats_elem = topic.select_one('.posts')
                      if stats_elem:
                          replies_match = re.search(r'(\d+)', stats_elem.text)
                          replies_count = int(replies_match.group(1)) if replies_match else 0
                      
                      views_elem = topic.select_one('.views')
                      if views_elem:
                          views_match = re.search(r'(\d+)', views_elem.text)
                          views_count = int(views_match.group(1)) if views_match else 0
                      
                      # Extract last post info
                      last_post_info = {
                          'author': '',
                          'date': '',
                          'url': ''
                      }
                      
                      last_post_elem = topic.select_one('.last-post')
                      if last_post_elem:
                          last_post_author_elem = last_post_elem.select_one('.username')
                          if last_post_author_elem:
                              last_post_info['author'] = last_post_author_elem.text.strip()
                          
                          last_post_date_elem = last_post_elem.select_one('time')
                          if last_post_date_elem:
                              last_post_info['date'] = last_post_date_elem.text.strip()
                          
                          last_post_link_elem = last_post_elem.select_one('a.lastpost-link')
                          if last_post_link_elem:
                              last_post_info['url'] = last_post_link_elem['href']
                              if not last_post_info['url'].startswith('http'):
                                  last_post_info['url'] = f"{self.base_url}/{last_post_info['url'].lstrip('./')}"
                      
                      topic_data = {
                          'id': topic_id,
                          'title': topic_title,
                          'url': topic_url,
                          'forum_id': forum_id,
                          'author': topic_author,
                          'date': topic_date,
                          'replies_count': replies_count,
                          'views_count': views_count,
                          'last_post': last_post_info
                      }
                      
                      self.topics.append(topic_data)
                  
                  # Check for pagination and scrape additional pages if needed
                  pagination = soup.select_one('.pagination')
                  if pagination:
                      current_page = 1
                      # For simplicity, we'll only scrape the first 3 pages of each forum
                      max_pages = 3
                      
                      while current_page < max_pages:
                          next_page_link = pagination.select_one(f'a[data-page="{current_page + 1}"]')
                          if not next_page_link:
                              break
                              
                          next_page_url = next_page_link['href']
                          if not next_page_url.startswith('http'):
                              next_page_url = f"{self.base_url}/{next_page_url.lstrip('./')}"
                          
                          print(f"Scraping forum page {current_page + 1}")
                          current_page += 1
                          
                          response = self.session.get(next_page_url)
                          soup = BeautifulSoup(response.text, 'html5lib')
                          
                          topics = soup.select('.topiclist.topics .row')
                          # Process topics (same code as above)
                          # For brevity, not repeating the same logic
                          
                          time.sleep(random.uniform(2, 5))  # Random delay
              
              def scrape_topic(self, topic_id, topic_url):
                  """Scrape a topic page to get posts"""
                  response = self.session.get(topic_url)
                  soup = BeautifulSoup(response.text, 'html5lib')
                  
                  # Extract posts
                  posts = soup.select('.post')
                  
                  for idx, post in enumerate(posts):
                      post_id_elem = post.get('id')
                      if not post_id_elem:
                          post_id = f"post_{topic_id}_{idx + 1}"
                      else:
                          post_id = post_id_elem.replace('p', '')
                      
                      # Extract post author
                      post_author = ""
                      post_author_elem = post.select_one('.username')
                      if post_author_elem:
                          post_author = post_author_elem.text.strip()
                      
                      # Extract author rank/title
                      author_rank = ""
                      author_rank_elem = post.select_one('.profile-rank')
                      if author_rank_elem:
                          author_rank = author_rank_elem.text.strip()
                      
                      # Extract post date
                      post_date = ""
                      post_date_elem = post.select_one('.author time')
                      if post_date_elem:
                          post_date = post_date_elem.text.strip()
                      
                      # Extract post content
                      post_content = ""
                      post_content_elem = post.select_one('.content')
                      if post_content_elem:
                          # Remove quoted content to avoid duplication
                          for quote in post_content_elem.select('.quotecontent'):
                              quote.decompose()
                          
                          post_content = str(post_content_elem)
                      
                      post_data = {
                          'id': post_id,
                          'topic_id': topic_id,
                          'author': post_author,
                          'author_rank': author_rank,
                          'date': post_date,
                          'content': post_content
                      }
                      
                      self.posts.append(post_data)
                  
                  # Check for pagination and scrape additional pages if needed
                  pagination = soup.select_one('.pagination')
                  if pagination:
                      current_page = 1
                      # For simplicity, we'll only scrape the first 3 pages of each topic
                      max_pages = 3
                      
                      while current_page < max_pages:
                          next_page_link = pagination.select_one(f'a[data-page="{current_page + 1}"]')
                          if not next_page_link:
                              break
                              
                          next_page_url = next_page_link['href']
                          if not next_page_url.startswith('http'):
                              next_page_url = f"{self.base_url}/{next_page_url.lstrip('./')}"
                          
                          print(f"Scraping topic page {current_page + 1}")
                          current_page += 1
                          
                          response = self.session.get(next_page_url)
                          soup = BeautifulSoup(response.text, 'html5lib')
                          
                          # Process posts (same code as above)
                          # For brevity, not repeating the same logic
                          
                          time.sleep(random.uniform(2, 5))  # Random delay
              
              def save_data(self):
                  """Save the scraped data to JSON file"""
                  forum_data = {
                      'categories': self.categories,
                      'forums': self.forums,
                      'topics': self.topics,
                      'posts': self.posts,
                      'metadata': {
                          'last_updated': datetime.now(self.timezone).isoformat(),
                          'total_categories': len(self.categories),
                          'total_forums': len(self.forums),
                          'total_topics': len(self.topics),
                          'total_posts': len(self.posts)
                      }
                  }
                  
                  with open(self.output_file, 'w', encoding='utf-8') as f:
                      json.dump(forum_data, f, ensure_ascii=False, indent=2)

          if __name__ == "__main__":
              # Create data directory if it doesn't exist
              os.makedirs('data', exist_ok=True)
              
              # Initialize and run the scraper
              scraper = IveltScraper('data/forum_data.json')
              scraper.scrape()
          EOF

      - name: Run scraper
        run: |
          python ivelt_scraper.py
      
      - name: Format JSON file for better browsing
        run: |
          python -c "import json; file_path='data/forum_data.json'; data=json.load(open(file_path, 'r', encoding='utf-8')); json.dump(data, open(file_path, 'w', encoding='utf-8'), ensure_ascii=False, indent=2)"
      
      - name: Create stats file
        run: |
          python -c "
          import json
          import datetime

          # Load forum data
          with open('data/forum_data.json', 'r', encoding='utf-8') as f:
              data = json.load(f)
          
          # Calculate statistics
          stats = {
              'total_categories': len(data['categories']),
              'total_forums': len(data['forums']),
              'total_topics': len(data['topics']),
              'total_posts': len(data['posts']),
              'last_updated': data['metadata']['last_updated'],
              'popular_forums': [],
              'active_topics': []
          }
          
          # Find most popular forums (by post count)
          forums_by_posts = sorted(data['forums'], key=lambda x: x['posts_count'], reverse=True)
          stats['popular_forums'] = forums_by_posts[:5]
          
          # Find most active topics (by reply count)
          topics_by_replies = sorted(data['topics'], key=lambda x: x['replies_count'], reverse=True)
          stats['active_topics'] = topics_by_replies[:10]
          
          # Save stats file
          with open('data/forum_stats.json', 'w', encoding='utf-8') as f:
              json.dump(stats, f, ensure_ascii=False, indent=2)
          "
      
      - name: Commit changes
        run: |
          git config --global user.name 'GitHub Action'
          git config --global user.email 'action@github.com'
          git add data/
          git add ivelt_scraper.py
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update forum data - $(date +'%Y-%m-%d %H:%M:%S')" && git push)
